{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b7a4eb",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e1172",
   "metadata": {},
   "source": [
    "- Dr. Sophia Suarez presented on the topic of image classification, specifically on the the supervised approach of analyzing an image and assigning it a label/category\n",
    "- Convolutional Neural Networks (CNNs) are a widely used model for this image classification problem. They combine convolutional layers and pooling layers to learn patterns, textures, shapes in images.\n",
    "- Dr. Suarez has been working in collaboration with the Ministry of Natural Resources and Forestry to classify images of zooplankton, which are useful indicators of the health of fresh water ecosystems.\n",
    "- She currently has a functioning model to classify images of the species samples. The next step for her work is to build a hierachical model, which classifies on different levels of the species hiearchy, i.e. colonial vs unicellular, spines vs no spines, etc. The purpose of this would be to build a model that allows flexible grouping of species at any classification level or resolution of choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a108b",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3009d",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/pytorch-image-classification-tutorial-for-beginners-94ea13f56f2/\n",
    "\n",
    "Note: \n",
    "Steps 1 through 5 are for training. Skip to step 6 to load model and run predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d687595",
   "metadata": {},
   "source": [
    "### 1. Create Conda Environment\n",
    "\n",
    "conda create -n torch_env python=3.10 -y\n",
    "\n",
    "conda activate torch_env\n",
    "\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "\n",
    "conda install numpy matplotlib pandas timm scikit-learn opencv -y\n",
    "\n",
    "pip install albumentations albumentations[imgaug] tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d405e93",
   "metadata": {},
   "source": [
    "### 2. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Image loader/reader\n",
    "import cv2\n",
    "\n",
    "# Pre-trained image model\n",
    "import timm\n",
    "\n",
    "# Sci-kit learn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "\n",
    "# And custom modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "from src.config import * # we get all of the model specifications\n",
    "from src.dataset import CustomDataset\n",
    "from src.plots import visualize_history\n",
    "from src.modeling.train import fit\n",
    "from src.modeling.predict import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39c8ed",
   "metadata": {},
   "source": [
    "### 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_folders = [\"Cheetahs\", \"Lions\"]\n",
    "labels = [0, 1]\n",
    "\n",
    "data = []\n",
    "\n",
    "for s, l in zip(sub_folders, labels):\n",
    "    folder_path = os.path.join(cfg.DATA_DIR, s)\n",
    "    print(folder_path)\n",
    "    for r, d, f in os.walk(folder_path):\n",
    "        for file in f:\n",
    "            if file.lower().endswith(\".jpg\"):\n",
    "                data.append((os.path.join(r, file), l))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['file_name','label'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b9061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "def drop_file(file_name):\n",
    "    file_name = os.path.join(cfg.DATA_DIR, file_name)\n",
    "    return df[df.file_name != file_name].reset_index(drop=True)\n",
    "\n",
    "def edit_label(file_name, new_value):\n",
    "    df.loc[df.file_name == file_name, \"label\"] = new_value\n",
    "    return df\n",
    "\n",
    "# these are incorrectly labeled\n",
    "df = edit_label(\"Cheetahs/055dbbcca8626dbb.jpg\", 1)\n",
    "df = edit_label(\"Cheetahs/01d688c043bdbfbb.jpg\", 1)\n",
    "\n",
    "# these are non cheetah/lion images\n",
    "files = [\n",
    "    \"Cheetahs\\\\02b086c4e96396f6.jpg\",\n",
    "    \"Lions\\\\0086c462d6a43b30.jpg\",\n",
    "    \"Lions\\\\002c2d9952ea0b75.jpg\",\n",
    "    \"Lions\\\\b551ce86336e1f38.jpg\",\n",
    "    \"Cheetahs\\\\0c20fa69621a2e6c.jpg\",\n",
    "    \"Cheetahs\\\\02dd8dd4344b04a7.jpg\",\n",
    "    \"Cheetahs\\\\52b64d96fc0647e8.jpg\",\n",
    "    \"Cheetahs\\\\02b086c4e96396f6.jpg\",\n",
    "    \"Lions\\\\00076a1ba8912d87.jpg\",\n",
    "    \"Lions\\\\003d62f84395408f.jpg\",\n",
    "    \"Cheetahs\\\\341bedaae1e7ad89.jpg\",\n",
    "    \"Lions\\\\0005dcb871947109.jpg\",\n",
    "    \"Cheetahs\\\\00707659aba29334.jpg\",\n",
    "    \"Cheetahs\\\\01750ba1a197e3ad.jpg\",\n",
    "    \"Cheetahs\\\\b2dda5f3f0bdad53.jpg\",\n",
    "    \"Lions\\\\05dcb92ec07ac597.jpg\",\n",
    "    \"Lions\\\\e58cc34f71331452.jpg\",\n",
    "    \"Cheetahs\\\\03e4a856df44695a.jpg\",\n",
    "    \"Lions\\\\0099aeca2bc9c585.jpg\",\n",
    "    \"Lions\\\\e830bd6011902b40.jpg\",\n",
    "    \"Cheetahs\\\\00d100b0231b60e6.jpg\",\n",
    "    \"Lions\\\\00ad4439a6573080.jpg\",\n",
    "    \"Lions\\\\00fac52924506248.jpg\",\n",
    "    \"Lions\\\\d153aaa9667d658a.jpg\",\n",
    "    \"Cheetahs\\\\02a5846a35629f1d.jpg\",\n",
    "    \"Cheetahs\\\\ebd1867dacf535c7.jpg\",\n",
    "    \"Lions\\\\00c5ba7d4683eddd.jpg\",\n",
    "    \"Lions\\\\0568d569fd07c96c.jpg\"\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    df = drop_file(f)\n",
    "    \n",
    "\n",
    "df = df.sample(frac=1, random_state=cfg.seed).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc68d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution- classes are balanced!\n",
    "sns.countplot(data = df, x = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some image samples\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "idx = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "\n",
    "        label = df.label[idx]\n",
    "        file_path = os.path.join(cfg.DATA_DIR, df.file_name[idx])\n",
    "        # print(file_path)\n",
    "        # print(i,j)\n",
    "\n",
    "        # Read an image with OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        # Convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize image\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "\n",
    "        ax[i,j].imshow(image)\n",
    "        ax[i,j].set_title(f\"Label: {label} ({'Lion' if label == 1 else 'Cheetah'})\")\n",
    "        ax[i,j].axis('off')\n",
    "        idx = idx+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a64ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some augmented data\n",
    "example_dataset = CustomDataset(cfg, df,image_size=256, transform=None,mode= \"train\")\n",
    "\n",
    "example_dataloader = DataLoader(example_dataset, \n",
    "                              batch_size = 32, \n",
    "                              shuffle = True, \n",
    "                              num_workers=0,\n",
    "                             )\n",
    "\n",
    "image_batch, label_batch = next(iter(example_dataloader))\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "idx = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "\n",
    "        image = image_batch[idx]\n",
    "        label = label_batch[idx]\n",
    "\n",
    "        # Reshape image\n",
    "        image = image.permute(1, 2, 0)\n",
    "\n",
    "        ax[i,j].imshow(image)\n",
    "        ax[i,j].set_title(f\"Image {idx}\\nLabel: {label} ({'Lion' if label == 1 else 'Cheetah'})\")\n",
    "        ax[i,j].axis('off')\n",
    "        idx = idx+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a51852",
   "metadata": {},
   "source": [
    "### 4. K-fold cross validation with random search hyperparameter optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set for reproducibility\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Keep track of best hyperparameter combination\n",
    "best_val_acc = 0\n",
    "best_params = None\n",
    "n_random_trials = 5  # number of random hyperparameter combinations to sample\n",
    "\n",
    "# Split data into train and test sets- hyperparameter optimization should only be done on train/validation sets\n",
    "train_df, test_df = train_test_split(df, \n",
    "                                      test_size = 0.1, \n",
    "                                      random_state = cfg.seed)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "for trial in range(n_random_trials):\n",
    "    print(f\"\\n=== Random Trial {trial+1}/{n_random_trials} ===\")\n",
    "    # Specify model\n",
    "    model = timm.create_model(cfg.backbone, pretrained=cfg.pretrained, num_classes=cfg.n_classes)\n",
    "    model = model.to(cfg.device)\n",
    "        \n",
    "    # Random sampling of hyperparameters\n",
    "    sampled_hp = sample_hyperparameters(cfg)\n",
    "    print(\"Sampled hyperparameters:\", sampled_hp)\n",
    "    \n",
    "    # Apply sampled hyperparameters to cfg\n",
    "    cfg = apply_hyperparameters(cfg, sampled_hp)\n",
    "    \n",
    "\n",
    "    fold_val_accs = []\n",
    "    fold_train_loss = []\n",
    "    fold_val_loss = []\n",
    "\n",
    "    # Assign each data image to a kfold class\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True,random_state=cfg.seed)\n",
    "    train_df[\"kfold\"] = -1 \n",
    "    for fold, (train_, val_) in enumerate(skf.split(X = train_df, y = train_df.label)):\n",
    "        train_df.loc[val_ , \"kfold\"] = fold\n",
    "\n",
    "\n",
    "    for fold in range(cfg.n_folds):\n",
    "        print(f\"\\n--- Fold {fold+1}/{cfg.n_folds} ---\\n\")\n",
    "\n",
    "        # Split data\n",
    "        train_df_fold = train_df[train_df.kfold != fold].reset_index(drop=True)\n",
    "        valid_df_fold = train_df[train_df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "        # Datasets\n",
    "        train_dataset = CustomDataset(cfg, train_df_fold, image_size= cfg.image_size, mode=\"train\")\n",
    "        valid_dataset = CustomDataset(cfg, valid_df_fold, image_size= cfg.image_size,mode=\"val\")\n",
    "\n",
    "        # Dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "        # Optimizer\n",
    "        if cfg.optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay, betas=cfg.betas)\n",
    "            lr_min = 0.1*cfg.learning_rate\n",
    "        if cfg.optimizer_type == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=cfg.learning_rate, momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n",
    "            lr_min = 0.01*cfg.learning_rate\n",
    "\n",
    "        # Scheduler- cosine decay to adapt the learning rate during the training process\n",
    "        total_steps = len(train_loader) * cfg.epochs\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=total_steps,\n",
    "            eta_min=lr_min\n",
    "        )\n",
    "\n",
    "\n",
    "        # Train\n",
    "        acc_list, loss_list, val_acc_list, val_loss_list, model, lrs = fit(\n",
    "            model, optimizer, scheduler, cfg, train_loader, valid_loader\n",
    "        )\n",
    "\n",
    "        fold_val_accs.append(val_acc_list[-1])\n",
    "        fold_train_loss.append(loss_list)\n",
    "        fold_val_loss.append(val_loss_list)\n",
    "\n",
    "    # Visualize training\n",
    "    visualize_history(cfg, fold_train_loss, fold_val_loss)\n",
    "\n",
    "    avg_val_acc = np.mean(fold_val_accs)\n",
    "    print(f\"Trial {trial+1} Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        best_params = sampled_hp\n",
    "    \n",
    "    del model, optimizer, scheduler\n",
    "    torch.cuda.empty_cache()        \n",
    "\n",
    "print(\"\\n=== Best Hyperparameters ===\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Validation Accuracy:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6ff79",
   "metadata": {},
   "source": [
    "### 5. Fine-tune on full training set with \"best\" hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ae027",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.epochs = 10\n",
    "train_dataset = CustomDataset(cfg, train_df,image_size=best_params['image_size'], mode= \"train\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                          batch_size = best_params[\"batch_size\"], \n",
    "                          shuffle = True, \n",
    "                          num_workers = 0,\n",
    "                         )\n",
    "\n",
    "model = timm.create_model(cfg.backbone, pretrained=cfg.pretrained, num_classes=cfg.n_classes)\n",
    "model = model.to(cfg.device)\n",
    "\n",
    "criterion = cfg.criterion\n",
    "\n",
    "\n",
    " # Optimizer\n",
    "if cfg.optimizer_type == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr_adam\"], weight_decay=best_params['weight_decay'], betas=best_params['betas'])\n",
    "    lr_min = 0.1*best_params[\"lr_adam\"]\n",
    "if cfg.optimizer_type == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=best_params[\"lr_sgd\"], momentum=best_params['momentum'], weight_decay=best_params['weight_decay'])\n",
    "    lr_min = 0.01*best_params[\"lr_sgd\"]\n",
    "\n",
    "# Scheduler- cosine decay to adapt the learning rate during the training process\n",
    "total_steps = len(train_loader) * best_params['epochs']\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps,\n",
    "    eta_min=lr_min\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "acc, loss, val_acc, val_loss, model, lrs = fit(model, optimizer, scheduler, cfg, train_dataloader)\n",
    "\n",
    "# Save down trained model and hyperparameters\n",
    "torch.save(model.state_dict(), os.path.join(cfg.OUTPUT_DIR,\"cheetah_lion_classifier_weights.pth\"))\n",
    "torch.save(best_params, os.path.join(cfg.OUTPUT_DIR, 'cheetah_lion_classifier_hyperparameters.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259b9ee",
   "metadata": {},
   "source": [
    "### 6. Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c888bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets- comment this out if notebook is run to train + test in one run\n",
    "train_df, test_df = train_test_split(df, \n",
    "                                      test_size = 0.1, \n",
    "                                      random_state = cfg.seed)\n",
    "\n",
    "# load model and hyperparameters\n",
    "model = timm.create_model(cfg.backbone, pretrained=cfg.pretrained, num_classes=cfg.n_classes)\n",
    "model = model.to(cfg.device)\n",
    "state_dict = torch.load(os.path.join(cfg.OUTPUT_DIR,\"cheetah_lion_classifier_weights.pth\"), weights_only=True, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "best_params = torch.load(os.path.join(cfg.OUTPUT_DIR, 'cheetah_lion_classifier_hyperparameters.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f72f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(cfg, test_df,image_size=best_params['image_size'], mode= \"test\")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                          batch_size = best_params[\"batch_size\"], \n",
    "                          shuffle = False, \n",
    "                          num_workers = 0,\n",
    "                         )\n",
    "predictions, metric = predict(test_dataloader, model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prediction'] = predictions\n",
    "test_df =  test_df.sample(frac=1, random_state=cfg.seed).reset_index(drop=True)\n",
    "\n",
    "for (idx, batch) in enumerate(test_dataloader):\n",
    "\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(10, 6))\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "\n",
    "            label = batch[1][idx]\n",
    "            image = batch[0][idx]\n",
    "            pred = predictions[idx]\n",
    "\n",
    "            # Reshape image\n",
    "            image = image.permute(1, 2, 0)\n",
    "\n",
    "            ax[i,j].imshow(image)\n",
    "            ax[i,j].set_title(f\"Ground Truth: {label} ({'Lion' if label == 1 else 'Cheetah'}) \\n Prediction: {pred} ({'Lion' if pred == 1 else 'Cheetah'}) \")#\\n{df.file_name[idx]}\")#, fontsize=14)\n",
    "            ax[i,j].axis('off')\n",
    "            idx = idx+1\n",
    "            \n",
    "            color = 'green' if label == pred else 'red'\n",
    "            ax[i,j].add_patch(Rectangle((0, 0), best_params['image_size'],best_params['image_size'],\n",
    "                      alpha=1, edgecolor=color, linewidth=5, fill=None))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c37988",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68539b1",
   "metadata": {},
   "source": [
    "We will use images of Calanoid and Daphnia to train model to classify zooplankton images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b944182",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_folders = [\"Calanoid\", \"Daphnia\"]\n",
    "labels = [0, 1]\n",
    "\n",
    "data = []\n",
    "\n",
    "for s, l in zip(sub_folders, labels):\n",
    "    folder_path = os.path.join(cfg.DATA_DIR, s)\n",
    "    print(folder_path)\n",
    "    for r, d, f in os.walk(folder_path):\n",
    "        for file in f:\n",
    "            if file.lower().endswith((\".tif\", \".tiff\")):\n",
    "                data.append((os.path.join(r, file), l))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['file_name','label'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20024181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution- classes are balanced!\n",
    "sns.countplot(data = df, x = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some image samples\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "# shuffle df\n",
    "df_sample = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "idx = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "\n",
    "        label = df_sample.label[idx]\n",
    "        file_path = os.path.join(cfg.DATA_DIR, df_sample.file_name[idx])\n",
    "\n",
    "        # Read an image with OpenCV\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        # Convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize image\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "\n",
    "        ax[i,j].imshow(image)\n",
    "        ax[i,j].set_title(f\"Label: {label} ({'Daphnia' if label == 1 else 'Calanoid'})\")\n",
    "        ax[i,j].axis('off')\n",
    "        idx = idx+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae777012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some augmented data\n",
    "example_dataset = CustomDataset(cfg, df,image_size=128, transform=None,mode= \"train\")\n",
    "\n",
    "example_dataloader = DataLoader(example_dataset, \n",
    "                              batch_size = 1, \n",
    "                              shuffle = True, \n",
    "                              num_workers=0,\n",
    "                             )\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        image_batch, label_batch = next(iter(example_dataloader))\n",
    "\n",
    "\n",
    "        image = image_batch[0]\n",
    "        label = label_batch[0]\n",
    "\n",
    "        # Reshape image\n",
    "        image = image.permute(1, 2, 0)\n",
    "\n",
    "        ax[i,j].imshow(image)\n",
    "        ax[i,j].set_title(f\"Label: {label} ({'Daphnia' if label == 1 else 'Calanoid'})\")\n",
    "        ax[i,j].axis('off')\n",
    "        idx = idx+1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207045a",
   "metadata": {},
   "source": [
    "### 4. K-fold cross validation with random search hyperparameter optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set reproducibility\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Keep track of best hyperparameter combination\n",
    "best_val_acc = 0\n",
    "best_params = None\n",
    "n_random_trials = 3  # number of random hyperparameter combinations to sample\n",
    "\n",
    "# Split data into train and test sets- hyperparameter optimization should only be done on train/validation sets\n",
    "train_df, test_df = train_test_split(df, \n",
    "                                      test_size = 0.2, \n",
    "                                      random_state = 42)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "for trial in range(n_random_trials):\n",
    "    print(f\"\\n=== Random Trial {trial+1}/{n_random_trials} ===\")\n",
    "    # Specify model\n",
    "    model = timm.create_model(cfg.backbone, pretrained=cfg.pretrained, num_classes=cfg.n_classes)\n",
    "    model = model.to(cfg.device)\n",
    "        \n",
    "    # Random sampling of hyperparameters\n",
    "    sampled_hp = sample_hyperparameters(cfg)\n",
    "    \n",
    "    # Apply sampled hyperparameters to cfg\n",
    "    cfg = apply_hyperparameters(cfg, sampled_hp)\n",
    "\n",
    "    # Force set batch size to 1 (we have few total samples)\n",
    "    cfg.batch_size = 1\n",
    "\n",
    "    # Force set epochs to 3 (don't wan't to overtrain on small set)\n",
    "    cfg.epochs = 3\n",
    "\n",
    "    \n",
    "    print(\"Sampled hyperparameters:\", sampled_hp)\n",
    "\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_val_loss = []\n",
    "    fold_val_accs = []\n",
    "\n",
    "    # Assign each data image to a kfold class\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True,random_state=cfg.seed)\n",
    "    train_df[\"kfold\"] = -1 \n",
    "    for fold, (train_, val_) in enumerate(skf.split(X = train_df, y = train_df.label)):\n",
    "        train_df.loc[val_ , \"kfold\"] = fold\n",
    "\n",
    "\n",
    "    for fold in range(cfg.n_folds):\n",
    "        print(f\"\\n--- Fold {fold+1}/{cfg.n_folds} ---\\n\")\n",
    "\n",
    "        # Split data\n",
    "        train_df_fold = train_df[train_df.kfold != fold].reset_index(drop=True)\n",
    "        valid_df_fold = train_df[train_df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "        # Datasets\n",
    "        train_dataset = CustomDataset(cfg, train_df_fold, image_size= cfg.image_size, mode=\"train\")\n",
    "        valid_dataset = CustomDataset(cfg, valid_df_fold, image_size= cfg.image_size,mode=\"val\")\n",
    "\n",
    "        # Dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
    "\n",
    "        # Optimizer\n",
    "        if cfg.optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay, betas=cfg.betas)\n",
    "            lr_min = 0.1*cfg.learning_rate\n",
    "        if cfg.optimizer_type == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=cfg.learning_rate, momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n",
    "            lr_min = 0.01*cfg.learning_rate\n",
    "\n",
    "        # Scheduler- cosine decay to adapt the learning rate during the training process\n",
    "        total_steps = len(train_loader) * cfg.epochs\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=total_steps,\n",
    "            eta_min=lr_min\n",
    "        )\n",
    "\n",
    "\n",
    "        # Train\n",
    "        acc_list, loss_list, val_acc_list, val_loss_list, model, lrs = fit(\n",
    "            model, optimizer, scheduler, cfg, train_loader, valid_loader\n",
    "        )\n",
    "\n",
    "        fold_val_accs.append(val_acc_list[-1])\n",
    "        fold_train_loss.append(loss_list)\n",
    "        fold_val_loss.append(val_loss_list)\n",
    "\n",
    "    # Visualize training\n",
    "    visualize_history(cfg, fold_train_loss, fold_val_loss)\n",
    "\n",
    "    avg_val_acc = np.mean(fold_val_accs)\n",
    "    print(f\"Trial {trial+1} Average Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        best_params = sampled_hp\n",
    "    \n",
    "    del model, optimizer, scheduler\n",
    "    torch.cuda.empty_cache()        \n",
    "\n",
    "print(\"\\n=== Best Hyperparameters ===\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Validation Accuracy:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b4dec3",
   "metadata": {},
   "source": [
    "### 5. Fine-tune on full training set with \"best\" hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747138f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(cfg, train_df,image_size=best_params['image_size'], mode= \"train\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                          batch_size = best_params[\"batch_size\"], \n",
    "                          shuffle = True, \n",
    "                          num_workers = 0,\n",
    "                         )\n",
    "\n",
    "model = timm.create_model(cfg.backbone, pretrained=cfg.pretrained, num_classes=cfg.n_classes)\n",
    "model = model.to(cfg.device)\n",
    "\n",
    "criterion = cfg.criterion\n",
    "\n",
    " # Optimizer\n",
    "if cfg.optimizer_type == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr_adam\"], weight_decay=best_params['weight_decay'], betas=best_params['betas'])\n",
    "    lr_min = 0.1*best_params[\"lr_adam\"]\n",
    "if cfg.optimizer_type == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=best_params[\"lr_sgd\"], momentum=best_params['momentum'], weight_decay=best_params['weight_decay'])\n",
    "    lr_min = 0.01*best_params[\"lr_sgd\"]\n",
    "\n",
    "# Scheduler- cosine decay to adapt the learning rate during the training process\n",
    "total_steps = len(train_loader) * best_params['epochs']\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_steps,\n",
    "    eta_min=lr_min\n",
    ")\n",
    "\n",
    "\n",
    "acc, loss, val_acc, val_loss, model, lrs = fit(model, optimizer, scheduler, cfg, train_dataloader)\n",
    "\n",
    "# Save down trained model\n",
    "torch.save(model.state_dict(), os.path.join(cfg.OUTPUT_DIR,\"daphnia_calanoid_classifier_weights.pth\"))\n",
    "torch.save(best_params, os.path.join(cfg.OUTPUT_DIR, 'daphnia_calanoid_hyperparameters.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a654fb",
   "metadata": {},
   "source": [
    "### 6. Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd593717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets- comment this out if notebook is run to train + test in one run\n",
    "train_df, test_df = train_test_split(df, \n",
    "                                      test_size = 0.1, \n",
    "                                      random_state = cfg.seed)\n",
    "\n",
    "# load model and hyperparameters\n",
    "model = timm.create_model(cfg.backbone, pretrained=cfg.pretrained, num_classes=cfg.n_classes)\n",
    "model = model.to(cfg.device)\n",
    "state_dict = torch.load(os.path.join(cfg.OUTPUT_DIR,\"daphnia_calanoid_classifier_weights.pth\"), weights_only=True, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "best_params = torch.load(os.path.join(cfg.OUTPUT_DIR, 'daphnia_calanoid_classifier_hyperparameters.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(cfg, test_df,image_size=best_params['image_size'], mode= \"test\")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                          batch_size = best_params[\"batch_size\"], \n",
    "                          shuffle = False, \n",
    "                          num_workers = 0,\n",
    "                         )\n",
    "predictions, metric = predict(test_dataloader, model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032847bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prediction'] = predictions\n",
    "test_df =  test_df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "for (idx, batch) in enumerate(test_dataloader):\n",
    "\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "\n",
    "            label = batch[1][idx]\n",
    "            image = batch[0][idx]\n",
    "            pred = predictions[idx]\n",
    "\n",
    "            # Reshape image\n",
    "            image = image.permute(1, 2, 0)\n",
    "\n",
    "            ax[i,j].imshow(image)\n",
    "\n",
    "            ax[i,j].set_title(f\"Ground Truth: {label} ({'Daphnia' if label == 1 else 'Calanoid'}) \\n Prediction: {pred} ({'Lion' if pred == 1 else 'Cheetah'}) \")#\\n{df.file_name[idx]}\")#, fontsize=14)\n",
    "            ax[i,j].axis('off')\n",
    "            idx = idx+1\n",
    "            \n",
    "            color = 'green' if label == pred else 'red'\n",
    "            ax[i,j].add_patch(Rectangle((0, 0), best_params['image_size'],best_params['image_size'],\n",
    "                      alpha=1, edgecolor=color, linewidth=5, fill=None))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
